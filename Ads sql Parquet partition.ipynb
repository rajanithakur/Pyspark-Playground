{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import findspark\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r0t00xk\n"
     ]
    }
   ],
   "source": [
    "USER = os.environ['USER']\n",
    "print(USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HADOOP_PATH = \"/opt/spark-2.3.0-bin-hadoop2.7\"\n",
    "HADOOP_JARS_PATH = HADOOP_PATH + \"/jars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(HADOOP_PATH)\n",
    "#import pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, ArrayType\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col,from_json,date_format\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_submit_args= \"--master yarn \\\n",
    "--deploy-mode client \\\n",
    "--executor-memory 6G \\\n",
    "--driver-memory 5G \\\n",
    "--queue queue2 \\\n",
    "--executor-cores 4 \\\n",
    "--num-executors 20 \\\n",
    "--conf spark.yarn.dist.archives=gs://p13n-storage2/\"+USER+\"/envs/dev27.zip#ecop27 \\\n",
    "--archives gs://p13n-storage2/r0t00xk/envs/dev27.zip#ecop27 \\\n",
    "--jars \"+HADOOP_JARS_PATH+\"/datanucleus-api-jdo-3.2.6.jar,\"+HADOOP_JARS_PATH+\"/datanucleus-rdbms-3.2.9.jar,\"+HADOOP_JARS_PATH+\"/datanucleus-core-3.2.10.jar,/u/users/r0t00xk/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.0.jar,/u/users/r0t00xk/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.1.0.jar \\\n",
    "pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "os.environ[\"PYSPARK_PYTHON\"] = './ecop27/dev27/bin/python'\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = './ecop27/dev27/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "  StructField(\"pageType\",StringType(),True),\n",
    "  StructField(\"pageContext\",StringType(),True),\n",
    "  StructField(\"deviceType\",StringType(),True),\n",
    "  StructField(\"details\",ArrayType(StringType()),True),\n",
    "  StructField(\"timestamp\",StringType(),True),\n",
    "  StructField(\"reqId\",StringType(),True),\n",
    "  StructField(\"betaCoeff\",StringType(),True),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paritiondate():\n",
    "  return datetime.today().strftime('%Y%m%d-%H%M')\n",
    "udfDateHRMM= udf(paritiondate, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Kafka Messages as spark structured stream\n",
    "df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\",\"kafka-1054180686-1-1268601033.wus.kafka-v2-sp-ad-server-prod.ms-df-messaging.prod-az-westus-23.prod.us.walmart.net:9092\").option(\"subscribe\", \"sp_p13n_request_logs\").option(\"multiLine\", \"true\").option(\"startingOffsets\", \"latest\").option(\"failOnDataLoss\",\"false\").load().selectExpr(\"CAST(value AS STRING) as kafkamsgval\").withColumn(\"hour\",udfDateHRMM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rootData: struct (nullable = true)\n",
      " |    |-- pageType: string (nullable = true)\n",
      " |    |-- pageContext: string (nullable = true)\n",
      " |    |-- deviceType: string (nullable = true)\n",
      " |    |-- details: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- timestamp: string (nullable = true)\n",
      " |    |-- reqId: string (nullable = true)\n",
      " |    |-- betaCoeff: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Kafka messages have their own schema, define own schema for storage and read kafka json as own defined schema\n",
    "dv = df.withColumn(\"rootData\",from_json(col(\"kafkamsgval\"),schema)) \\\n",
    "                   .select(\"rootData\")\n",
    "dv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pageType: string (nullable = true)\n",
      " |-- pageContext: string (nullable = true)\n",
      " |-- deviceType: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- reqId: string (nullable = true)\n",
      " |-- betaCoeff: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rootData is one main json blob, read that into individual columns\n",
    "mainDF= dv.select(\"rootData.pageType\", \"rootData.pageContext\", \"rootData.deviceType\", \"rootData.timestamp\", \"rootData.reqId\",\"rootData.betaCoeff\")\n",
    "# from pyspark.sql.functions import *\n",
    "# mainDF.withColumn('hour', hour(from_unixtime('timestamp'))).show()\n",
    "mainDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark sql init, create table adlogs from dataframe\n",
    "mainDF.createTempView(\"adlogs\")\n",
    "#you can execute sql like queries like below\n",
    "df2 = spark.sql(\"select * from adlogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "u'Job aborted.\\n=== Streaming Query ===\\nIdentifier: [id = 225ef25a-312a-4b46-97d3-ef201458eeb8, runId = f9b39040-82f5-4466-b0b1-71e32481cc8a]\\nCurrent Committed Offsets: {KafkaSource[Subscribe[sp_p13n_request_logs]]: {\"sp_p13n_request_logs\":{\"8\":108878023,\"11\":108887676,\"2\":108885204,\"5\":108893843,\"14\":108897080,\"13\":108888518,\"4\":108887228,\"7\":108900061,\"1\":108889302,\"10\":108878878,\"9\":108898059,\"3\":108869338,\"12\":111953736,\"6\":108901414,\"0\":108901352}}}\\nCurrent Available Offsets: {KafkaSource[Subscribe[sp_p13n_request_logs]]: {\"sp_p13n_request_logs\":{\"8\":111541505,\"11\":111550786,\"2\":111547509,\"5\":111555540,\"14\":111559869,\"13\":111551683,\"4\":111550388,\"7\":111563848,\"1\":111553036,\"10\":111541259,\"9\":111560083,\"3\":111532753,\"12\":113459953,\"6\":111563658,\"0\":111562650}}}\\n\\nCurrent State: ACTIVE\\nThread State: RUNNABLE\\n\\nLogical Plan:\\nProject [kafkamsgval#21, paritiondate() AS hour#23]\\n+- Project [cast(value#8 as string) AS kafkamsgval#21]\\n   +- StreamingExecutionRelation KafkaSource[Subscribe[sp_p13n_request_logs]], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-44218ee40a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconsoleoutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1 seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hour'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"gs://p13n-storage2/r0t00xk/dump\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"truncate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"false\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.execution.QueryExecutionException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: u'Job aborted.\\n=== Streaming Query ===\\nIdentifier: [id = 225ef25a-312a-4b46-97d3-ef201458eeb8, runId = f9b39040-82f5-4466-b0b1-71e32481cc8a]\\nCurrent Committed Offsets: {KafkaSource[Subscribe[sp_p13n_request_logs]]: {\"sp_p13n_request_logs\":{\"8\":108878023,\"11\":108887676,\"2\":108885204,\"5\":108893843,\"14\":108897080,\"13\":108888518,\"4\":108887228,\"7\":108900061,\"1\":108889302,\"10\":108878878,\"9\":108898059,\"3\":108869338,\"12\":111953736,\"6\":108901414,\"0\":108901352}}}\\nCurrent Available Offsets: {KafkaSource[Subscribe[sp_p13n_request_logs]]: {\"sp_p13n_request_logs\":{\"8\":111541505,\"11\":111550786,\"2\":111547509,\"5\":111555540,\"14\":111559869,\"13\":111551683,\"4\":111550388,\"7\":111563848,\"1\":111553036,\"10\":111541259,\"9\":111560083,\"3\":111532753,\"12\":113459953,\"6\":111563658,\"0\":111562650}}}\\n\\nCurrent State: ACTIVE\\nThread State: RUNNABLE\\n\\nLogical Plan:\\nProject [kafkamsgval#21, paritiondate() AS hour#23]\\n+- Project [cast(value#8 as string) AS kafkamsgval#21]\\n   +- StreamingExecutionRelation KafkaSource[Subscribe[sp_p13n_request_logs]], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13]\\n'"
     ]
    }
   ],
   "source": [
    "consoleoutput = df.writeStream.trigger(processingTime='1 seconds').partitionBy('hour').format(\"parquet\").option(\"path\",\"gs://p13n-storage2/r0t00xk/dump\").outputMode(\"append\").option(\"checkpointLocation\", \"checkpoint\").option(\"truncate\", \"false\").start().awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.streams.awaitAnyTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
