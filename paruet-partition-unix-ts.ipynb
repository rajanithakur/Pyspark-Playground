{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "HADOOP_PATH = \"/opt/spark-2.3.0-bin-hadoop2.7\"\n",
    "HADOOP_JARS_PATH = HADOOP_PATH + \"/jars/\"\n",
    "import findspark\n",
    "findspark.init(HADOOP_PATH)\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, ArrayType\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col,from_json,date_format\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r0t00xk\n"
     ]
    }
   ],
   "source": [
    "USER = os.environ['USER']\n",
    "print(USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_submit_args= \"--master yarn \\\n",
    "--deploy-mode client \\\n",
    "--executor-memory 6G \\\n",
    "--driver-memory 5G \\\n",
    "--queue queue2 \\\n",
    "--executor-cores 4 \\\n",
    "--num-executors 20 \\\n",
    "--conf spark.yarn.dist.archives=gs://p13n-storage2/\"+USER+\"/envs/dev27.zip#ecop27 \\\n",
    "--archives gs://p13n-storage2/r0t00xk/envs/dev27.zip#ecop27 \\\n",
    "--jars \"+HADOOP_JARS_PATH+\"/datanucleus-api-jdo-3.2.6.jar,\"+HADOOP_JARS_PATH+\"/datanucleus-rdbms-3.2.9.jar,\"+HADOOP_JARS_PATH+\"/datanucleus-core-3.2.10.jar,/u/users/r0t00xk/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.0.jar,/u/users/r0t00xk/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.1.0.jar \\\n",
    "pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "os.environ[\"PYSPARK_PYTHON\"] = './ecop27/dev27/bin/python'\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = './ecop27/dev27/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "  StructField(\"pageType\",StringType(),True),\n",
    "  StructField(\"pageContext\",StringType(),True),\n",
    "  StructField(\"deviceType\",StringType(),True),\n",
    "  StructField(\"details\",ArrayType(StringType()),True),\n",
    "  StructField(\"timestamp\",StringType(),True),\n",
    "  StructField(\"reqId\",StringType(),True),\n",
    "  StructField(\"betaCoeff\",StringType(),True),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Kafka Messages as spark structured stream\n",
    "df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\",\"kafka-1054180686-1-1268601033.wus.kafka-v2-sp-ad-server-prod.ms-df-messaging.prod-az-westus-23.prod.us.walmart.net:9092\").option(\"subscribe\", \"sp_p13n_request_logs\").option(\"multiLine\", \"true\").option(\"startingOffsets\", \"latest\").option(\"failOnDataLoss\",\"false\").load().selectExpr(\"CAST(value AS STRING) as kafkamsgval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rootData: struct (nullable = true)\n",
      " |    |-- pageType: string (nullable = true)\n",
      " |    |-- pageContext: string (nullable = true)\n",
      " |    |-- deviceType: string (nullable = true)\n",
      " |    |-- details: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- timestamp: string (nullable = true)\n",
      " |    |-- reqId: string (nullable = true)\n",
      " |    |-- betaCoeff: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Kafka messages have their own schema, define own schema for storage and read kafka json as own defined schema\n",
    "dv = df.withColumn(\"rootData\",from_json(col(\"kafkamsgval\"),schema)) \\\n",
    "                   .select(\"rootData\")\n",
    "dv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pageType: string (nullable = true)\n",
      " |-- pageContext: string (nullable = true)\n",
      " |-- deviceType: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- reqId: string (nullable = true)\n",
      " |-- betaCoeff: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rootData is one main json blob, read that into individual columns\n",
    "mainDF= dv.select(\"rootData.pageType\", \"rootData.pageContext\", \"rootData.deviceType\", \"rootData.timestamp\", \"rootData.reqId\",\"rootData.betaCoeff\")\n",
    "# from pyspark.sql.functions import *\n",
    "# mainDF.withColumn('hour', hour(from_unixtime('timestamp'))).show()\n",
    "mainDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark sql init, create table adlogs from dataframe\n",
    "mainDF.createTempView(\"adlogs\")\n",
    "#you can execute sql like queries like below\n",
    "df2 = spark.sql(\"select * from adlogs\")\n",
    "\n",
    "# df.withColumn(\"hour\", hour(F.to_timestamp(\"timestamp\",\"dd/MM/yyyy HH:mm:ss\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consoleoutput = df2.withColumn('hour', hour(from_unixtime(unix_timestamp()))).writeStream.trigger(processingTime='1 seconds').partitionBy(\"hour\").format(\"parquet\").option(\"path\",\"gs://p13n-storage2/r0t00xk/dump\").outputMode(\"append\").option(\"checkpointLocation\", \"checkpoint\").option(\"truncate\", \"false\").start().awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.streams.awaitAnyTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
